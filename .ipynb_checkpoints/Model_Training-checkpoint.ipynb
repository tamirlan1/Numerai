{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, preprocessing, linear_model\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "competition = '53'\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the data from the CSV files\n",
    "training_data = pd.read_csv('numerai_datasets' + competition + '/numerai_training_data.csv', header=0)\n",
    "prediction_data = pd.read_csv('numerai_datasets' + competition + '/numerai_tournament_data.csv', header=0)\n",
    "\n",
    "\n",
    "# Transform the loaded CSV data into numpy arrays\n",
    "features = [f for f in list(training_data) if \"feature\" in f]\n",
    "X = training_data[features]\n",
    "Y = training_data[\"target\"]\n",
    "x_prediction = prediction_data[features]\n",
    "x_validation = prediction_data[prediction_data['data_type'] == 'validation'][features]\n",
    "y_validation = prediction_data[\"target\"][prediction_data['data_type'] == 'validation']\n",
    "ids = prediction_data[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "model = linear_model.LogisticRegression(n_jobs=-1)\n",
    "# model = RandomForestClassifier(n_estimators=1000, max_features=0.5, max_depth=20, min_samples_split=20, random_state=0, n_jobs=-1)\n",
    "print(\"Training...\")\n",
    "# Your model is trained on the training_data\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = log_loss(Y, pd.DataFrame(model.predict_proba(X)).as_matrix())\n",
    "print 'Train Loss:', train_loss\n",
    "val_loss = log_loss(y_validation, pd.DataFrame(model.predict_proba(x_validation)).as_matrix())\n",
    "print 'Validation Loss:', val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Predicting...\")\n",
    "# Your trained model is now used to make predictions on the numerai_tournament_data\n",
    "# The model returns two columns: [probability of 0, probability of 1]\n",
    "# We are just interested in the probability that the target is 1.\n",
    "y_prediction = model.predict_proba(x_prediction)\n",
    "results = y_prediction[:, 1]\n",
    "results_df = pd.DataFrame(data={'probability':results})\n",
    "joined = pd.DataFrame(ids).join(results_df)\n",
    "\n",
    "print(\"Writing predictions to predictions.csv\")\n",
    "# Save the predictions out to a CSV file\n",
    "joined.to_csv('numerai_datasets' + competition + '/predictions_LogReg.csv\", index=False)\n",
    "# Now you can upload these predictions on numer.ai\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "group = training_data.era.apply(lambda x: int(x[3:]))\n",
    "cv_split = group_kfold.split(X, Y, groups)\n",
    "# for a, b in group_kfold.split(X, y, groups):\n",
    "#     print 'train', a, X[a], y[a]\n",
    "#     print 'test', b, X[b], y[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object split at 0x7ffab0d3a870>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [108405, 108405, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-569f5c66fcc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \"\"\"\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [108405, 108405, 4]"
     ]
    }
   ],
   "source": [
    "for a, b in cv_split:\n",
    "    print 'train', a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8ced5652ad44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# >>> y = np.array([0, 0, 1, 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# >>> test_fold = [0, 1, -1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredefinedSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# print ps.get_n_splits()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_splits'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "# >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "# >>> y = np.array([0, 0, 1, 1])\n",
    "# >>> test_fold = [0, 1, -1, 1]\n",
    "ps = PredefinedSplit(group)\n",
    "# print ps.get_n_splits()\n",
    "pss = list(ps.split())\n",
    "# print(ps)       \n",
    "# PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
    "# for train_index, test_index in ps.split():\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "clf = RandomForestClassifier()\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": range(5,15),\n",
    "              \"max_features\": ['sqrt',0.2, 0.3, 0.4, 0.5],\n",
    "              \"n_estimators\": [300, 400, 500, 600, 700, 800],\n",
    "             \"min_samples_split\": [100, 300, 400, 500, 600, 800, 1000],\n",
    "             \"min_samples_leaf\": [20, 40, 50, 70 ,100]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=pss, n_jobs=-1, scoring=log_scoring, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 96 folds for each of 100 candidates, totalling 9600 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-99d357ce1332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n\u001b[1;32m      4\u001b[0m       \" parameter settings.\" % ((time() - start), n_iter_search))\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# report(random_search.cv_results_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                                           random_state=self.random_state)\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "search = random_search.fit(X, Y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "# report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -0.69292, std: 0.00047, params: {'n_estimators': 500, 'min_samples_split': 800, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 40},\n",
       " mean: -0.69297, std: 0.00053, params: {'n_estimators': 800, 'min_samples_split': 500, 'max_depth': 9, 'max_features': 0.4, 'min_samples_leaf': 100},\n",
       " mean: -0.69335, std: 0.00071, params: {'n_estimators': 600, 'min_samples_split': 400, 'max_depth': 14, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69291, std: 0.00042, params: {'n_estimators': 300, 'min_samples_split': 100, 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 50},\n",
       " mean: -0.69292, std: 0.00037, params: {'n_estimators': 400, 'min_samples_split': 600, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 20},\n",
       " mean: -0.69311, std: 0.00062, params: {'n_estimators': 500, 'min_samples_split': 500, 'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 20},\n",
       " mean: -0.69291, std: 0.00043, params: {'n_estimators': 500, 'min_samples_split': 800, 'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69293, std: 0.00046, params: {'n_estimators': 800, 'min_samples_split': 500, 'max_depth': 6, 'max_features': 0.5, 'min_samples_leaf': 20},\n",
       " mean: -0.69290, std: 0.00048, params: {'n_estimators': 300, 'min_samples_split': 1000, 'max_depth': 7, 'max_features': 0.4, 'min_samples_leaf': 100},\n",
       " mean: -0.69354, std: 0.00074, params: {'n_estimators': 400, 'min_samples_split': 100, 'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 20},\n",
       " mean: -0.69291, std: 0.00044, params: {'n_estimators': 600, 'min_samples_split': 800, 'max_depth': 7, 'max_features': 0.3, 'min_samples_leaf': 100},\n",
       " mean: -0.69330, std: 0.00073, params: {'n_estimators': 500, 'min_samples_split': 300, 'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 20},\n",
       " mean: -0.69292, std: 0.00042, params: {'n_estimators': 700, 'min_samples_split': 100, 'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69294, std: 0.00050, params: {'n_estimators': 800, 'min_samples_split': 1000, 'max_depth': 10, 'max_features': 0.2, 'min_samples_leaf': 70},\n",
       " mean: -0.69311, std: 0.00058, params: {'n_estimators': 400, 'min_samples_split': 300, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 100},\n",
       " mean: -0.69298, std: 0.00051, params: {'n_estimators': 700, 'min_samples_split': 600, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69289, std: 0.00037, params: {'n_estimators': 400, 'min_samples_split': 300, 'max_depth': 5, 'max_features': 0.3, 'min_samples_leaf': 40},\n",
       " mean: -0.69309, std: 0.00058, params: {'n_estimators': 800, 'min_samples_split': 500, 'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 50},\n",
       " mean: -0.69290, std: 0.00047, params: {'n_estimators': 600, 'min_samples_split': 400, 'max_depth': 6, 'max_features': 0.4, 'min_samples_leaf': 100},\n",
       " mean: -0.69294, std: 0.00043, params: {'n_estimators': 700, 'min_samples_split': 1000, 'max_depth': 6, 'max_features': 0.4, 'min_samples_leaf': 20},\n",
       " mean: -0.69296, std: 0.00051, params: {'n_estimators': 700, 'min_samples_split': 500, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 20},\n",
       " mean: -0.69290, std: 0.00039, params: {'n_estimators': 600, 'min_samples_split': 100, 'max_depth': 5, 'max_features': 0.3, 'min_samples_leaf': 100},\n",
       " mean: -0.69308, std: 0.00061, params: {'n_estimators': 800, 'min_samples_split': 300, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69290, std: 0.00044, params: {'n_estimators': 300, 'min_samples_split': 400, 'max_depth': 7, 'max_features': 0.2, 'min_samples_leaf': 20},\n",
       " mean: -0.69288, std: 0.00035, params: {'n_estimators': 600, 'min_samples_split': 1000, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69293, std: 0.00048, params: {'n_estimators': 500, 'min_samples_split': 500, 'max_depth': 9, 'max_features': 'sqrt', 'min_samples_leaf': 40},\n",
       " mean: -0.69290, std: 0.00045, params: {'n_estimators': 400, 'min_samples_split': 500, 'max_depth': 8, 'max_features': 0.2, 'min_samples_leaf': 40},\n",
       " mean: -0.69299, std: 0.00056, params: {'n_estimators': 800, 'min_samples_split': 1000, 'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 100},\n",
       " mean: -0.69319, std: 0.00067, params: {'n_estimators': 700, 'min_samples_split': 500, 'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 20},\n",
       " mean: -0.69318, std: 0.00060, params: {'n_estimators': 500, 'min_samples_split': 300, 'max_depth': 13, 'max_features': 'sqrt', 'min_samples_leaf': 20},\n",
       " mean: -0.69311, std: 0.00067, params: {'n_estimators': 500, 'min_samples_split': 600, 'max_depth': 12, 'max_features': 0.5, 'min_samples_leaf': 20},\n",
       " mean: -0.69300, std: 0.00057, params: {'n_estimators': 600, 'min_samples_split': 500, 'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 40},\n",
       " mean: -0.69328, std: 0.00068, params: {'n_estimators': 400, 'min_samples_split': 100, 'max_depth': 12, 'max_features': 0.3, 'min_samples_leaf': 100},\n",
       " mean: -0.69291, std: 0.00042, params: {'n_estimators': 500, 'min_samples_split': 400, 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69292, std: 0.00045, params: {'n_estimators': 300, 'min_samples_split': 500, 'max_depth': 6, 'max_features': 0.4, 'min_samples_leaf': 40},\n",
       " mean: -0.69301, std: 0.00061, params: {'n_estimators': 700, 'min_samples_split': 600, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 20},\n",
       " mean: -0.69308, std: 0.00058, params: {'n_estimators': 500, 'min_samples_split': 300, 'max_depth': 10, 'max_features': 0.3, 'min_samples_leaf': 100},\n",
       " mean: -0.69301, std: 0.00057, params: {'n_estimators': 400, 'min_samples_split': 100, 'max_depth': 8, 'max_features': 0.4, 'min_samples_leaf': 20},\n",
       " mean: -0.69295, std: 0.00053, params: {'n_estimators': 600, 'min_samples_split': 100, 'max_depth': 7, 'max_features': 0.4, 'min_samples_leaf': 40},\n",
       " mean: -0.69291, std: 0.00045, params: {'n_estimators': 800, 'min_samples_split': 1000, 'max_depth': 7, 'max_features': 0.3, 'min_samples_leaf': 40},\n",
       " mean: -0.69299, std: 0.00063, params: {'n_estimators': 600, 'min_samples_split': 1000, 'max_depth': 11, 'max_features': 0.5, 'min_samples_leaf': 20},\n",
       " mean: -0.69305, std: 0.00055, params: {'n_estimators': 500, 'min_samples_split': 400, 'max_depth': 11, 'max_features': 'sqrt', 'min_samples_leaf': 40},\n",
       " mean: -0.69289, std: 0.00039, params: {'n_estimators': 700, 'min_samples_split': 600, 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 50},\n",
       " mean: -0.69290, std: 0.00038, params: {'n_estimators': 800, 'min_samples_split': 800, 'max_depth': 5, 'max_features': 0.3, 'min_samples_leaf': 100},\n",
       " mean: -0.69320, std: 0.00064, params: {'n_estimators': 700, 'min_samples_split': 400, 'max_depth': 13, 'max_features': 0.3, 'min_samples_leaf': 70},\n",
       " mean: -0.69296, std: 0.00053, params: {'n_estimators': 400, 'min_samples_split': 500, 'max_depth': 8, 'max_features': 0.5, 'min_samples_leaf': 40},\n",
       " mean: -0.69288, std: 0.00040, params: {'n_estimators': 800, 'min_samples_split': 500, 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69313, std: 0.00063, params: {'n_estimators': 400, 'min_samples_split': 600, 'max_depth': 13, 'max_features': 0.3, 'min_samples_leaf': 100},\n",
       " mean: -0.69291, std: 0.00049, params: {'n_estimators': 800, 'min_samples_split': 800, 'max_depth': 9, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69289, std: 0.00040, params: {'n_estimators': 700, 'min_samples_split': 500, 'max_depth': 6, 'max_features': 'sqrt', 'min_samples_leaf': 70},\n",
       " mean: -0.69294, std: 0.00051, params: {'n_estimators': 600, 'min_samples_split': 400, 'max_depth': 7, 'max_features': 0.5, 'min_samples_leaf': 40},\n",
       " mean: -0.69295, std: 0.00048, params: {'n_estimators': 600, 'min_samples_split': 800, 'max_depth': 10, 'max_features': 0.2, 'min_samples_leaf': 100},\n",
       " mean: -0.69288, std: 0.00040, params: {'n_estimators': 600, 'min_samples_split': 300, 'max_depth': 5, 'max_features': 0.4, 'min_samples_leaf': 100},\n",
       " mean: -0.69290, std: 0.00043, params: {'n_estimators': 600, 'min_samples_split': 500, 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 20},\n",
       " mean: -0.69340, std: 0.00070, params: {'n_estimators': 600, 'min_samples_split': 100, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 50},\n",
       " mean: -0.69297, std: 0.00051, params: {'n_estimators': 700, 'min_samples_split': 500, 'max_depth': 8, 'max_features': 0.4, 'min_samples_leaf': 50},\n",
       " mean: -0.69314, std: 0.00063, params: {'n_estimators': 300, 'min_samples_split': 500, 'max_depth': 13, 'max_features': 0.3, 'min_samples_leaf': 50},\n",
       " mean: -0.69303, std: 0.00059, params: {'n_estimators': 300, 'min_samples_split': 500, 'max_depth': 10, 'max_features': 0.4, 'min_samples_leaf': 100},\n",
       " mean: -0.69310, std: 0.00064, params: {'n_estimators': 600, 'min_samples_split': 800, 'max_depth': 14, 'max_features': 0.3, 'min_samples_leaf': 50},\n",
       " mean: -0.69291, std: 0.00045, params: {'n_estimators': 800, 'min_samples_split': 400, 'max_depth': 6, 'max_features': 0.4, 'min_samples_leaf': 70},\n",
       " mean: -0.69290, std: 0.00035, params: {'n_estimators': 400, 'min_samples_split': 800, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69288, std: 0.00038, params: {'n_estimators': 400, 'min_samples_split': 500, 'max_depth': 6, 'max_features': 0.2, 'min_samples_leaf': 100},\n",
       " mean: -0.69302, std: 0.00057, params: {'n_estimators': 600, 'min_samples_split': 400, 'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 70},\n",
       " mean: -0.69320, std: 0.00069, params: {'n_estimators': 300, 'min_samples_split': 600, 'max_depth': 14, 'max_features': 0.4, 'min_samples_leaf': 40},\n",
       " mean: -0.69308, std: 0.00057, params: {'n_estimators': 300, 'min_samples_split': 100, 'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 100},\n",
       " mean: -0.69322, std: 0.00067, params: {'n_estimators': 600, 'min_samples_split': 100, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69295, std: 0.00054, params: {'n_estimators': 600, 'min_samples_split': 800, 'max_depth': 9, 'max_features': 0.4, 'min_samples_leaf': 20},\n",
       " mean: -0.69314, std: 0.00063, params: {'n_estimators': 300, 'min_samples_split': 300, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69291, std: 0.00045, params: {'n_estimators': 700, 'min_samples_split': 600, 'max_depth': 6, 'max_features': 0.4, 'min_samples_leaf': 50},\n",
       " mean: -0.69291, std: 0.00042, params: {'n_estimators': 400, 'min_samples_split': 400, 'max_depth': 6, 'max_features': 0.3, 'min_samples_leaf': 20},\n",
       " mean: -0.69313, std: 0.00064, params: {'n_estimators': 400, 'min_samples_split': 800, 'max_depth': 12, 'max_features': 0.5, 'min_samples_leaf': 100},\n",
       " mean: -0.69294, std: 0.00042, params: {'n_estimators': 300, 'min_samples_split': 400, 'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 40},\n",
       " mean: -0.69295, std: 0.00051, params: {'n_estimators': 700, 'min_samples_split': 1000, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 50},\n",
       " mean: -0.69290, std: 0.00037, params: {'n_estimators': 700, 'min_samples_split': 300, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 70},\n",
       " mean: -0.69335, std: 0.00073, params: {'n_estimators': 400, 'min_samples_split': 400, 'max_depth': 14, 'max_features': 0.5, 'min_samples_leaf': 70},\n",
       " mean: -0.69308, std: 0.00059, params: {'n_estimators': 600, 'min_samples_split': 400, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 100},\n",
       " mean: -0.69316, std: 0.00069, params: {'n_estimators': 600, 'min_samples_split': 800, 'max_depth': 14, 'max_features': 0.5, 'min_samples_leaf': 70},\n",
       " mean: -0.69328, std: 0.00062, params: {'n_estimators': 500, 'min_samples_split': 300, 'max_depth': 14, 'max_features': 'sqrt', 'min_samples_leaf': 70},\n",
       " mean: -0.69293, std: 0.00046, params: {'n_estimators': 800, 'min_samples_split': 400, 'max_depth': 8, 'max_features': 0.2, 'min_samples_leaf': 40},\n",
       " mean: -0.69290, std: 0.00036, params: {'n_estimators': 300, 'min_samples_split': 100, 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 100},\n",
       " mean: -0.69298, std: 0.00057, params: {'n_estimators': 700, 'min_samples_split': 1000, 'max_depth': 10, 'max_features': 0.4, 'min_samples_leaf': 50},\n",
       " mean: -0.69292, std: 0.00048, params: {'n_estimators': 500, 'min_samples_split': 1000, 'max_depth': 7, 'max_features': 0.4, 'min_samples_leaf': 50},\n",
       " mean: -0.69330, std: 0.00064, params: {'n_estimators': 600, 'min_samples_split': 300, 'max_depth': 14, 'max_features': 0.2, 'min_samples_leaf': 50},\n",
       " mean: -0.69295, std: 0.00048, params: {'n_estimators': 600, 'min_samples_split': 300, 'max_depth': 9, 'max_features': 0.2, 'min_samples_leaf': 20},\n",
       " mean: -0.69318, std: 0.00064, params: {'n_estimators': 800, 'min_samples_split': 400, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 40},\n",
       " mean: -0.69326, std: 0.00067, params: {'n_estimators': 300, 'min_samples_split': 300, 'max_depth': 13, 'max_features': 0.2, 'min_samples_leaf': 50},\n",
       " mean: -0.69289, std: 0.00046, params: {'n_estimators': 300, 'min_samples_split': 1000, 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 20},\n",
       " mean: -0.69297, std: 0.00056, params: {'n_estimators': 500, 'min_samples_split': 1000, 'max_depth': 10, 'max_features': 0.4, 'min_samples_leaf': 20},\n",
       " mean: -0.69300, std: 0.00054, params: {'n_estimators': 400, 'min_samples_split': 600, 'max_depth': 9, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69294, std: 0.00043, params: {'n_estimators': 400, 'min_samples_split': 400, 'max_depth': 6, 'max_features': 0.4, 'min_samples_leaf': 50},\n",
       " mean: -0.69299, std: 0.00048, params: {'n_estimators': 400, 'min_samples_split': 100, 'max_depth': 8, 'max_features': 0.2, 'min_samples_leaf': 20},\n",
       " mean: -0.69312, std: 0.00067, params: {'n_estimators': 700, 'min_samples_split': 500, 'max_depth': 12, 'max_features': 0.5, 'min_samples_leaf': 70},\n",
       " mean: -0.69300, std: 0.00052, params: {'n_estimators': 800, 'min_samples_split': 400, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 70},\n",
       " mean: -0.69293, std: 0.00047, params: {'n_estimators': 600, 'min_samples_split': 800, 'max_depth': 9, 'max_features': 'sqrt', 'min_samples_leaf': 50},\n",
       " mean: -0.69292, std: 0.00047, params: {'n_estimators': 600, 'min_samples_split': 600, 'max_depth': 9, 'max_features': 0.2, 'min_samples_leaf': 100},\n",
       " mean: -0.69297, std: 0.00044, params: {'n_estimators': 300, 'min_samples_split': 100, 'max_depth': 7, 'max_features': 0.3, 'min_samples_leaf': 40},\n",
       " mean: -0.69301, std: 0.00060, params: {'n_estimators': 600, 'min_samples_split': 1000, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 50},\n",
       " mean: -0.69313, std: 0.00064, params: {'n_estimators': 700, 'min_samples_split': 600, 'max_depth': 13, 'max_features': 0.4, 'min_samples_leaf': 40},\n",
       " mean: -0.69296, std: 0.00052, params: {'n_estimators': 500, 'min_samples_split': 800, 'max_depth': 9, 'max_features': 0.3, 'min_samples_leaf': 20},\n",
       " mean: -0.69305, std: 0.00059, params: {'n_estimators': 400, 'min_samples_split': 500, 'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 70}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6928779206980246,\n",
       " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=5, max_features=0.4, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=100,\n",
       "             min_samples_split=300, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=600, n_jobs=1, oob_score=False, random_state=None,\n",
       "             verbose=0, warm_start=False),\n",
       " {'max_depth': 5,\n",
       "  'max_features': 0.4,\n",
       "  'min_samples_leaf': 100,\n",
       "  'min_samples_split': 300,\n",
       "  'n_estimators': 600})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_, search.best_estimator_, search.best_params_\n",
    "\n",
    "# (-0.6928779206980246,\n",
    "#  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "#              max_depth=5, max_features=0.4, max_leaf_nodes=None,\n",
    "#              min_impurity_split=1e-07, min_samples_leaf=100,\n",
    "#              min_samples_split=300, min_weight_fraction_leaf=0.0,\n",
    "#              n_estimators=600, n_jobs=1, oob_score=False, random_state=None,\n",
    "#              verbose=0, warm_start=False),\n",
    "#  {'max_depth': 5,\n",
    "#   'max_features': 0.4,\n",
    "#   'min_samples_leaf': 100,\n",
    "#   'min_samples_split': 300,\n",
    "#   'n_estimators': 600})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = search.best_estimator_.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# model = RandomForestClassifier(n_estimators=10, max_features='sqrt', max_depth=10, min_samples_split=20, random_state=0, n_jobs=-1)\n",
    "# print(\"Training...\")\n",
    "# # Your model is trained on the training_data\n",
    "# model.fit(X, Y)\n",
    "# Train Loss: 0.67897489267\n",
    "# Validation Loss: 0.694670352826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.690883866474\n",
      "Validation Loss: 0.692585525566\n"
     ]
    }
   ],
   "source": [
    "train_loss = log_loss(Y, pd.DataFrame(model.predict_proba(X)).as_matrix())\n",
    "print 'Train Loss:', train_loss\n",
    "val_loss = log_loss(y_validation, pd.DataFrame(model.predict_proba(x_validation)).as_matrix())\n",
    "print 'Validation Loss:', val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n",
      "Writing predictions to predictions.csv\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting...\")\n",
    "# Your trained model is now used to make predictions on the numerai_tournament_data\n",
    "# The model returns two columns: [probability of 0, probability of 1]\n",
    "# We are just interested in the probability that the target is 1.\n",
    "y_prediction = model.predict_proba(x_prediction)\n",
    "results = y_prediction[:, 1]\n",
    "results_df = pd.DataFrame(data={'probability':results})\n",
    "joined = pd.DataFrame(ids).join(results_df)\n",
    "\n",
    "print(\"Writing predictions to predictions.csv\")\n",
    "# Save the predictions out to a CSV file\n",
    "joined.to_csv('numerai_datasets' + competition + '/predictions_RandomForest_100search.csv', index=False)\n",
    "# Now you can upload these predictions on numer.ai\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "clf = GradientBoostingClassifier()\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "              \"max_features\": ['sqrt',0.2, 0.3, 0.4, 0.5],\n",
    "              \"n_estimators\": [300, 400, 500, 600, 700, 800],\n",
    "              \"learning_rate\": [round(i,2) for i in np.arange(0.05,0.2,0.01)],\n",
    "             \"min_samples_split\": [100, 300, 400, 500, 600, 800, 1000],\n",
    "             \"min_samples_leaf\": [20, 40, 50, 70 ,100]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, cv=cv, n_jobs=-1, scoring=log_scoring, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'ShuffleSplit' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-99d357ce1332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n\u001b[1;32m      4\u001b[0m       \" parameter settings.\" % ((time() - start), n_iter_search))\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# report(random_search.cv_results_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                                           random_state=self.random_state)\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tseidakhmetov/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mn_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n\u001b[0;32m--> 558\u001b[0;31m                       \" {2} fits\".format(len(cv), n_candidates,\n\u001b[0m\u001b[1;32m    559\u001b[0m                                          n_candidates * len(cv)))\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'ShuffleSplit' has no len()"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "search = random_search.fit(X, Y)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "# report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -0.69681, std: 0.00157, params: {'learning_rate': 0.09, 'min_samples_leaf': 50, 'n_estimators': 600, 'max_features': 0.4, 'min_samples_split': 1000, 'max_depth': 6},\n",
       " mean: -0.69971, std: 0.00170, params: {'learning_rate': 0.12, 'min_samples_leaf': 40, 'n_estimators': 700, 'max_features': 0.3, 'min_samples_split': 300, 'max_depth': 5},\n",
       " mean: -0.69482, std: 0.00106, params: {'learning_rate': 0.16, 'min_samples_leaf': 50, 'n_estimators': 400, 'max_features': 'sqrt', 'min_samples_split': 600, 'max_depth': 3},\n",
       " mean: -0.69761, std: 0.00127, params: {'learning_rate': 0.18, 'min_samples_leaf': 100, 'n_estimators': 500, 'max_features': 'sqrt', 'min_samples_split': 500, 'max_depth': 4},\n",
       " mean: -0.69373, std: 0.00089, params: {'learning_rate': 0.09, 'min_samples_leaf': 40, 'n_estimators': 400, 'max_features': 0.2, 'min_samples_split': 800, 'max_depth': 3},\n",
       " mean: -0.69363, std: 0.00085, params: {'learning_rate': 0.07, 'min_samples_leaf': 20, 'n_estimators': 800, 'max_features': 0.2, 'min_samples_split': 100, 'max_depth': 2},\n",
       " mean: -0.69430, std: 0.00108, params: {'learning_rate': 0.11, 'min_samples_leaf': 50, 'n_estimators': 800, 'max_features': 0.4, 'min_samples_split': 800, 'max_depth': 2},\n",
       " mean: -0.69634, std: 0.00130, params: {'learning_rate': 0.08, 'min_samples_leaf': 100, 'n_estimators': 400, 'max_features': 0.4, 'min_samples_split': 800, 'max_depth': 7},\n",
       " mean: -0.70127, std: 0.00195, params: {'learning_rate': 0.15, 'min_samples_leaf': 20, 'n_estimators': 600, 'max_features': 'sqrt', 'min_samples_split': 800, 'max_depth': 6},\n",
       " mean: -0.70846, std: 0.00228, params: {'learning_rate': 0.19, 'min_samples_leaf': 20, 'n_estimators': 400, 'max_features': 0.3, 'min_samples_split': 400, 'max_depth': 8}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "x = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "groups = np.array([0, 0, 2, 2])\n",
    "group_kfold = GroupKFold(n_splits=2)\n",
    "group_kfold.get_n_splits(x, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TRAIN:', array([0, 1]), 'TEST:', array([2, 3]))\n",
      "('TRAIN:', array([2, 3]), 'TEST:', array([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in group_kfold.split(x, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
